set.seed(420)
x = rnorm(100, mean = 0 , sd = 1)
par(mfrow = c(1, 2))
qqnorm(x, col = "darkgrey")
qqline(x, lty = 2, lwd = 2, col = "dodgerblue")
qq_plot(x)
par(mfrow = c(1, 3))
set.seed(420)
qq_plot(rnorm(10))
qq_plot(rnorm(25))
qq_plot(rnorm(100))
par(mfrow = c(1, 3))
set.seed(420)
qq_plot(rt(10, df = 4))
qq_plot(rt(25, df = 4))
qq_plot(rt(100, df = 4))
# Exponentialpar(mfrow = c(1, 3))
set.seed(420)
qq_plot(rexp(10))
qq_plot(rexp(25))
qq_plot(rexp(100))
qqnorm(resid(fit_1), main = "Normal Q-Q Plot, fit_1", col = "darkgrey")
qqline(resid(fit_1), col = "dodgerblue", lwd = 2)
set.seed(42)
shapiro.test(rnorm(25))
shapiro.test(rexp(25))
shapiro.test(resid(fit_1))
shapiro.test(resid(fit_2))
shapiro.test(resid(fit_3))
par(mfrow = c(1, 3))
set.seed(42)
ex_data  = data.frame(x = 1:10,
y = 10:1 + rnorm(n = 10))
ex_model = lm(y ~ x, data = ex_data)
# low leverage, large residual, small influence
point_1 = c(5.4, 11)
ex_data_1 = rbind(ex_data, point_1)
model_1 = lm(y ~ x, data = ex_data_1)
plot(y ~ x, data = ex_data_1, cex = 2, pch = 20, col = "grey",
main = "Low Leverage, Large Residual, Small Influence")
points(x = point_1[1], y = point_1[2], pch = 1, cex = 4, col = "black", lwd = 2)
abline(ex_model, col = "dodgerblue", lwd = 2)
abline(model_1, lty = 2, col = "darkorange", lwd = 2)
legend("bottomleft", c("Original Data", "Added Point"),
lty = c(1, 2), col = c("dodgerblue", "darkorange"))
# high leverage, small residual, small influence
point_2 = c(18, -5.7)
ex_data_2 = rbind(ex_data, point_2)
model_2 = lm(y ~ x, data = ex_data_2)
plot(y ~ x, data = ex_data_2, cex = 2, pch = 20, col = "grey",
main = "High Leverage, Small Residual, Small Influence")
points(x = point_2[1], y = point_2[2], pch = 1, cex = 4, col = "black", lwd = 2)
abline(ex_model, col = "dodgerblue", lwd = 2)
abline(model_2, lty = 2, col = "darkorange", lwd = 2)
legend("bottomleft", c("Original Data", "Added Point"),
lty = c(1, 2), col = c("dodgerblue", "darkorange"))
# high leverage, large residual, large influence
point_3 = c(14, 5.1)
ex_data_3 = rbind(ex_data, point_3)
model_3 = lm(y ~ x, data = ex_data_3)
plot(y ~ x, data = ex_data_3, cex = 2, pch = 20, col = "grey", ylim = c(-3, 12),
main = "High Leverage, Large Residual, Large Influence")
points(x = point_3[1], y = point_3[2], pch = 1, cex = 4, col = "black", lwd = 2)
abline(ex_model, col = "dodgerblue", lwd = 2)
abline(model_3, lty = 2, col = "darkorange", lwd = 2)
legend("bottomleft", c("Original Data", "Added Point"),
lty = c(1, 2), col = c("dodgerblue", "darkorange"))
coef(ex_model)[2]
coef(model_1)[2]
coef(model_2)[2]
coef(model_2)[2]
coef(model_3)[2]
lev_ex = data.frame(
x1 = c(0, 11, 11, 7, 4, 10, 5, 8),
x2 = c(1, 5, 4, 3, 1, 4, 4, 2),
y  = c(11, 15, 13, 14, 0, 19, 16, 8))
plot(x2 ~ x1, data = lev_ex, cex = 2)
points(7, 3, pch = 20, col = "red", cex = 2)
X = cbind(rep(1, 8), lev_ex$x1, lev_ex$x2)
H = X %*% solve(t(X) %*% X) %*% t(X)
diag(H)
sum(diag(H))
lev_fit = lm(y ~ ., data = lev_ex)
hatvalues(lev_fit)
coef(lev_fit)
which.max(hatvalues(lev_fit))
lev_ex[which.max(hatvalues(lev_fit)),]
lev_ex_1 = lev_ex
lev_ex_1$y[1] = 20
lm(y ~ ., data = lev_ex_1)
resid(model_1)
rstandard(model_1)
rstandard(model_1)[abs(rstandard(model_1)) > 2]
resid(model_2)
rstandard(model_2)
rstandard(model_2)[abs(rstandard(model_2)) > 2]
resid(model_3)
rstandard(model_3)
rstandard(model_3)[abs(rstandard(model_3)) > 2]
cooks.distance(model_1)[11] > 4 / length(cooks.distance(model_1))
cooks.distance(model_2)[11] > 4 / length(cooks.distance(model_2))
cooks.distance(model_3)[11] > 4 / length(cooks.distance(model_3))
mpg_hp_add = lm(mpg ~ hp + am, data = mtcars)
mpg_hp_add = lm(mpg ~ hp + am, data = mtcars)
plot(fitted(mpg_hp_add), resid(mpg_hp_add), col = "grey", pch = 20,
xlab = "Fitted", ylab = "Residual",
main = "mtcars: Fitted versus Residuals")
abline(h = 0, col = "darkorange", lwd = 2)
# BP test w
bptest(mpg_hp_add)
qqnorm(resid(mpg_hp_add), col = "darkgrey")
qqline(resid(mpg_hp_add), col = "dodgerblue", lwd = 2)
shapiro.test(resid(mpg_hp_add))
sum(hatvalues(mpg_hp_add) > 2 * mean(hatvalues(mpg_hp_add)))
sum(abs(rstandard(mpg_hp_add)) > 2)
cd_mpg_hp_add = cooks.distance(mpg_hp_add)
sum(cd_mpg_hp_add > 4 / length(cd_mpg_hp_add))
large_cd_mpg = cd_mpg_hp_add > 4 / length(cd_mpg_hp_add)
cd_mpg_hp_add[large_cd_mpg]
coef(mpg_hp_add)
coef(mpg_hp_add)
# let's drop and compare the coef with an without these cars
mpg_hp_add_fix = lm(mpg ~ hp + am,
data = mtcars,
subset = cd_mpg_hp_add <= 4 / length(cd_mpg_hp_add))
coef(mpg_hp_add_fix)
par(mfrow = c(2, 2))
plot(mpg_hp_add)
DataName = data.frame(Y = c(y1,y2,y3,y4,y5),X = c(x1,x2,x3,x4,x5))
DataName = data.frame(Y = c(3,2,2,1,1),X = c(8,5,6,5,2))
DataName = data.frame(Y = c(3,2,2,1,1),X = c(8,5,6,5,2))
lm(Y~X, data = DataName)
summary(reg)
reg = lm(Y~X, data = DataName)
summary(reg)
DataName = data.frame(Y = c(4,18,4,1,12),X = c(6,8,4,3,4))
reg = lm(Y~X, data = DataName)
summary(reg)
DataName = data.frame(Y = c(4,18,4,1,12),X = c(6,8,4,3,4))
reg = lm(Y~X, data = DataName)
summary(reg)
DataName = data.frame(Y = c(3,2,2,1,1),X = c(8,5,6,5,2))
reg = lm(Y~X, data = DataName)
summary(reg)
setwd('/Users/rileylatham/Downloads/SSC442/Lab_Working_Files/Lab_3')
ameslist <- read.table("https://msudataanalytics.github.io/SSC442/Labs/data/ames.csv",
header = TRUE,
sep = ",")
ameslist <- read.table("https://msudataanalytics.github.io/SSC442/Labs/data/ames.csv",
header = TRUE,
sep = ",")
library(tidyverse)
library(ggplot2)
library(scales)
ameslist = ameslist[ , !(names(ameslist) %in% c('OverallCond', 'OverallQual'))]
View(ameslist)
# Read in the data set with headers TRUE
ameslist <- read.table("https://msudataanalytics.github.io/SSC442/Labs/data/ames.csv",
header = TRUE,
sep = ",")
int_type = lapply(ameslist, class)
# Generates a dataframe from ames data with all integer type data
Ames = ameslist[int_type=='integer']
# Drop columns that don't have meaning
Ames$MSSubClass = NULL
Ames = Ames[ , !(names(Ames) %in% c('OverallCond', 'OverallQual'))]
View(Ames)
lm.fit = lm(SalePrice ~ GrLivArea, data=Ames)
lm.fit
rmse = function(actual, predicted) {
sqrt(mean((actual - predicted) ^ 2))
}
rmse(SalePrice, lm.fit)
rmse(Ames$SalePrice, lm.fit)
lm.fit.GrLivArea = lm(SalePrice ~ GrLivArea, data=Ames)
lm.fit.GrLivArea
lm.fit.GrLivArea = lm(SalePrice ~ GrLivArea + LotArea, data=Ames)
lm.fit.Comp2
lm.fit.Comp2 = lm(SalePrice ~ GrLivArea + LotArea, data=Ames)
lm.fit.Comp2
lm.fit.Comp3 = lm(SalePrice ~ GrLivArea + LotArea + MasVnrArea, data=Ames)
lm.fit.Comp3
lm.fit.Comp5 = lm(SalePrice ~ GrLivArea + LotArea + MasVnrArea + TotalBsmtSF +
TotRmsAbvGrd, data=Ames)
lm.fit.Comp5
lm.fit.Comp10 = lm(SalePrice ~ GrLivArea + LotArea + MasVnrArea + TotalBsmtSF + TotRmsAbvGrd +
GarageArea + X1stFlrSF + X2ndFlrSF + FullBath + Fireplaces, data=Ames)
lm.fit.Comp10
lm.fit.Comp15 = lm(SalePrice ~ GrLivArea + LotArea + MasVnrArea + TotalBsmtSF + TotRmsAbvGrd +
GarageArea + X1stFlrSF + X2ndFlrSF + FullBath + Fireplaces +
HalfBath + BedroomAbvGrd + GarageCars + YearBuilt + WoodDeckSF, data=Ames)
lm.fit.Comp15 = lm(SalePrice ~ GrLivArea + LotArea + MasVnrArea + TotalBsmtSF + TotRmsAbvGrd +
GarageArea + X1stFlrSF + X2ndFlrSF + FullBath + Fireplaces +
HalfBath + BedroomAbvGr + GarageCars + YearBuilt + WoodDeckSF, data=Ames)
lm.fit.Comp15
lm.fit.Comp3 = lm(SalePrice ~ GrLivArea + TotRmsAbvGrd + MasVnrArea, data=Ames)
lm.fit.Comp1
lm.fit.Comp3
lm.fit.Comp5 = lm(SalePrice ~ GrLivArea + LotArea + MasVnrArea + TotalBsmtSF +
TotRmsAbvGrd, data=Ames)
lm.fit.Comp5
get_complexity(lm.fit.Comp15)
get_complexity = function(model) {
length(coef(model)) - 1
}
get_complexity(lm.fit.Comp15)
rmse(Amse$SalePrice, predict(lm.fit.comp15))
rmse(Ames$SalePrice, predict(lm.fit.comp15))
rmse(Ames$SalePrice, predict(lm.fit.Comp15))
rmse(Ames$SalePrice, predict.lm((lm.fit.Comp15)))
predict(lm.fit.Comp15)
rmse(Ames$SalePrice, predictions)
predictions = predict(lm.fit.Comp15, Ames)
rmse(Ames$SalePrice, predictions)
# Split our data into train and test
test = Ames[1:1095]
# Split our data into train and test
test = Ames[ , 1:1095]
# Split our data into train and test
test = Ames[1:1095, ]
# Split our data into train and test
train = Ames[1:1095, ]
test = Ames[1096:1460, ]
train = Ames[1:1095, ]
test = Ames[1096:1460, ]
# Makes forward selection plots up to complexity 15
lm.fit.Comp1 = lm(SalePrice ~ GrLivArea, data=train)
lm.fit.Comp1
lm.fit.Comp3 = lm(SalePrice ~ GrLivArea + TotRmsAbvGrd + MasVnrArea, data=train)
lm.fit.Comp3
lm.fit.Comp5 = lm(SalePrice ~ GrLivArea + LotArea + MasVnrArea + TotalBsmtSF +
TotRmsAbvGrd, data=train)
lm.fit.Comp5
lm.fit.Comp10 = lm(SalePrice ~ GrLivArea + LotArea + MasVnrArea + TotalBsmtSF + TotRmsAbvGrd +
GarageArea + X1stFlrSF + X2ndFlrSF + FullBath + Fireplaces, data=train)
lm.fit.Comp10
lm.fit.Comp15 = lm(SalePrice ~ GrLivArea + LotArea + MasVnrArea + TotalBsmtSF + TotRmsAbvGrd +
GarageArea + X1stFlrSF + X2ndFlrSF + FullBath + Fireplaces +
HalfBath + BedroomAbvGr + GarageCars + YearBuilt + WoodDeckSF, data=train)
lm.fit.Comp15
predictions = predict(lm.fit.Comp15, test)
rmse(Ames$SalePrice, predictions)
rmse(test$SalePrice, predictions)
summary(predictions)
RMSE <- function(error) { sqrt(mean(error^2)) }
RMSE(lm.fit.Comp15$residuals)
names(lm.fit.Comp15)
predictions = predict(lm.fit.Comp15, test, na.action = na.pass)
rmse(test$SalePrice, predictions)
test$SalePrice
predictions
predictions = predict(lm.fit.Comp15, data = test, na.action = na.pass)
predictions
count(test$SalePrice)
rmse(test$SalePrice, predictions)
length(test$SalePrice)
length(predictions)
train = Ames[1:1095, ]
test = Ames[1096:1460, ]
predictions = predict(lm.fit.Comp15, newdata = test, na.action = na.pass)
length(predictions)
rmse(test$SalePrice, predictions)
length(predictions)
length(test$SalePrice)
type(rmse(test$SalePrice, predictions))
typeof(rmse(test$SalePrice, predictions))
print(rmse(test$SalePrice, predictions))
names(rmse(test$SalePrice, predictions))
View(train)
# This code included solely for completeness; you do not need to execute this.
x = seq(0, 100, by = 0.001)
f = function(x) {
((x - 50) / 50) ^ 2 + 2
}
g = function(x) {
1 - ((x - 50) / 50)
}
par(mgp = c(1.5, 1.5, 0))
plot(x, g(x), ylim = c(0, 3), type = "l", lwd = 2,
ylab = "Error", xlab = expression(Low %<-% Complexity %->% High),
main = "Error versus Model Complexity", col = "darkorange",
axes = FALSE)
grid()
axis(1, labels = FALSE)
axis(2, labels = FALSE)
box()
curve(f, lty = 6, col = "dodgerblue", lwd = 3, add = TRUE)
legend("bottomleft", c("(Expected) Test", "Train"), lty = c(6, 1), lwd = 3,
col = c("dodgerblue", "darkorange"))
# This code included solely for completeness; you do not need to execute this.
x = seq(0.01, 0.99, length.out = 1000)
par(mfrow = c(1, 3))
par(mgp = c(1.5, 1.5, 0))
b = 0.05 / x
v = 5 * x ^ 2 + 0.5
bayes = 4
epe = b + v + bayes
plot(x, b, type = "l", ylim = c(0, 10), col = "dodgerblue", lwd = 2, lty = 3,
xlab = "Model Complexity", ylab = "Error", axes = FALSE,
main = "More Dominant Variance")
axis(1, labels = FALSE)
axis(2, labels = FALSE)
grid()
box()
lines(x, v, col = "darkorange", lwd = 2, lty = 4)
lines(x, epe, col = "black", lwd = 2)
abline(h = bayes, lty = 2, lwd = 2, col = "darkgrey")
abline(v = x[which.min(epe)], col = "grey", lty = 3, lwd = 2)
b = 0.05 / x
v = 5 * x ^ 4 + 0.5
bayes = 4
epe = b + v + bayes
plot(x, b, type = "l", ylim = c(0, 10), col = "dodgerblue", lwd = 2, lty = 3,
xlab = "Model Complexity", ylab = "Error", axes = FALSE,
main = "Decomposition of Prediction Error")
axis(1, labels = FALSE)
axis(2, labels = FALSE)
grid()
box()
lines(x, v, col = "darkorange", lwd = 2, lty = 4)
lines(x, epe, col = "black", lwd = 2)
abline(h = bayes, lty = 2, lwd = 2, col = "darkgrey")
abline(v = x[which.min(epe)], col = "grey", lty = 3, lwd = 2)
b = 6 - 6 * x ^ (1 / 4)
v = 5 * x ^ 6 + 0.5
bayes = 4
epe = b + v + bayes
plot(x, b, type = "l", ylim = c(0, 10), col = "dodgerblue", lwd = 2, lty = 3,
xlab = "Model Complexity", ylab = "Error", axes = FALSE,
main = "More Dominant Bias")
axis(1, labels = FALSE)
axis(2, labels = FALSE)
grid()
box()
lines(x, v, col = "darkorange", lwd = 2, lty = 4)
lines(x, epe, col = "black", lwd = 2)
abline(h = bayes, lty = 2, lwd = 2, col = "darkgrey")
abline(v = x[which.min(epe)], col = "grey", lty = 3, lwd = 2)
legend("topright", c("Squared Bias", "Variance", "Bayes", "EPE"), lty = c(3, 4, 2, 1),
col = c("dodgerblue", "darkorange", "darkgrey", "black"), lwd = 2)
f = function(x) {
x ^ 2
}
get_sim_data = function(f, sample_size = 100) {
x = runif(n = sample_size, min = 0, max = 1)
y = rnorm(n = sample_size, mean = f(x), sd = 0.3)
data.frame(x, y)
}
get_sim_data = function(f, sample_size = 100) {
x = runif(n = sample_size, min = 0, max = 1)
eps = rnorm(n = sample_size, mean = 0, sd = 0.75)
y = f(x) + eps
data.frame(x, y)
}
set.seed(1)
sim_data = get_sim_data(f)
fit_0 = lm(y ~ 1,                   data = sim_data)
fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)
set.seed(42)
plot(y ~ x, data = sim_data, col = "grey", pch = 20,
main = "Four Polynomial Models fit to a Simulated Dataset")
grid()
grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, f(grid), col = "black", lwd = 3)
lines(grid, predict(fit_0, newdata = data.frame(x = grid)), col = "dodgerblue",  lwd = 2, lty = 2)
lines(grid, predict(fit_1, newdata = data.frame(x = grid)), col = "firebrick",   lwd = 2, lty = 3)
lines(grid, predict(fit_2, newdata = data.frame(x = grid)), col = "springgreen", lwd = 2, lty = 4)
lines(grid, predict(fit_9, newdata = data.frame(x = grid)), col = "darkorange",  lwd = 2, lty = 5)
legend("topleft",
c("y ~ 1", "y ~ poly(x, 1)", "y ~ poly(x, 2)",  "y ~ poly(x, 9)", "truth"),
col = c("dodgerblue", "firebrick", "springgreen", "darkorange", "black"), lty = c(2, 3, 4, 5, 1), lwd = 2)
legend("topleft", c("y ~ 1", "y ~ poly(x, 9)"), col = c("dodgerblue", "darkorange"), lty = c(2, 5), lwd = 2)
set.seed(430)
sim_data_1 = get_sim_data(f)
sim_data_2 = get_sim_data(f)
sim_data_3 = get_sim_data(f)
fit_0_1 = lm(y ~ 1, data = sim_data_1)
fit_0_2 = lm(y ~ 1, data = sim_data_2)
fit_0_3 = lm(y ~ 1, data = sim_data_3)
fit_9_1 = lm(y ~ poly(x, degree = 9), data = sim_data_1)
fit_9_2 = lm(y ~ poly(x, degree = 9), data = sim_data_2)
fit_9_3 = lm(y ~ poly(x, degree = 9), data = sim_data_3)
plot(y ~ x, data = sim_data_1, col = "grey", pch = 20, main = "Simulated Dataset 1")
grid()
grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, predict(fit_0_1, newdata = data.frame(x = grid)), col = "dodgerblue", lwd = 2, lty = 2)
lines(grid, predict(fit_9_1, newdata = data.frame(x = grid)), col = "darkorange", lwd = 2, lty = 5)
legend("topleft", c("y ~ 1", "y ~ poly(x, 9)"), col = c("dodgerblue", "darkorange"), lty = c(2, 5), lwd = 2)
plot(y ~ x, data = sim_data_2, col = "grey", pch = 20, main = "Simulated Dataset 2")
grid()
grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, predict(fit_0_2, newdata = data.frame(x = grid)), col = "dodgerblue", lwd = 2, lty = 2)
lines(grid, predict(fit_9_2, newdata = data.frame(x = grid)), col = "darkorange", lwd = 2, lty = 5)
legend("topleft", c("y ~ 1", "y ~ poly(x, 9)"), col = c("dodgerblue", "darkorange"), lty = c(2, 5), lwd = 2)
plot(y ~ x, data = sim_data_3, col = "grey", pch = 20, main = "Simulated Dataset 3")
grid()
grid = seq(from = 0, to = 2, by = 0.01)
lines(grid, predict(fit_0_3, newdata = data.frame(x = grid)), col = "dodgerblue", lwd = 2, lty = 2)
lines(grid, predict(fit_9_3, newdata = data.frame(x = grid)), col = "darkorange", lwd = 2, lty = 5)
legend("topleft", c("y ~ 1", "y ~ poly(x, 9)"), col = c("dodgerblue", "darkorange"), lty = c(2, 5), lwd = 2)
par(mfrow = c(1, 3))
# if you're reading this code
# it's BAD! don't use it. (or clean it up)
# also, note to self: clean up this code!!!
grid = seq(from = 0, to = 2, by = 0.01)
set.seed(430)
sim_data_1 = get_sim_data(f)
sim_data_2 = get_sim_data(f)
sim_data_3 = get_sim_data(f)
fit_0_1 = FNN::knn.reg(train = sim_data_1["x"], test = data.frame(x = grid), y = sim_data_1["y"], k = 5)$pred
fit_0_2 = FNN::knn.reg(train = sim_data_2["x"], test = data.frame(x = grid), y = sim_data_2["y"], k = 5)$pred
fit_0_3 = FNN::knn.reg(train = sim_data_3["x"], test = data.frame(x = grid), y = sim_data_3["y"], k = 5)$pred
fit_9_1 = FNN::knn.reg(train = sim_data_1["x"], test = data.frame(x = grid), y = sim_data_1["y"], k = 100)$pred
fit_9_2 = FNN::knn.reg(train = sim_data_2["x"], test = data.frame(x = grid), y = sim_data_2["y"], k = 100)$pred
fit_9_3 = FNN::knn.reg(train = sim_data_3["x"], test = data.frame(x = grid), y = sim_data_3["y"], k = 100)$pred
plot(y ~ x, data = sim_data_1, col = "grey", pch = 20, main = "Simulated Dataset 1")
grid()
lines(grid, fit_0_1, col = "dodgerblue", lwd = 1, lty = 1)
lines(grid, fit_9_1, col = "darkorange", lwd = 2, lty = 2)
legend("topleft", c("k = 5", "k = 100"), col = c("dodgerblue", "darkorange"), lty = c(1, 2), lwd = 2)
plot(y ~ x, data = sim_data_2, col = "grey", pch = 20, main = "Simulated Dataset 2")
grid()
lines(grid, fit_0_2, col = "dodgerblue", lwd = 1, lty = 1)
lines(grid, fit_9_2, col = "darkorange", lwd = 2, lty = 2)
legend("topleft", c("k = 5", "k = 100"), col = c("dodgerblue", "darkorange"), lty = c(1, 2), lwd = 2)
plot(y ~ x, data = sim_data_3, col = "grey", pch = 20, main = "Simulated Dataset 3")
grid()
lines(grid, fit_0_3, col = "dodgerblue", lwd = 1, lty = 1)
lines(grid, fit_9_3, col = "darkorange", lwd = 2, lty = 2)
legend("topleft", c("k = 5", "k = 100"), col = c("dodgerblue", "darkorange"), lty = c(1, 2), lwd = 2)
set.seed(1)
n_sims = 250
n_models = 4
x = data.frame(x = 0.90) # Fixed point at which we make predictions
predictions = matrix(0, nrow = n_sims, ncol = n_models)
for (sim in 1:n_sims) {
# Simulate new, random, training data
# This is the only random portion of the bias, var, and mse calculations
# This allows us to calculate the expectation over D
sim_data = get_sim_data(f)
# Fit models
fit_0 = lm(y ~ 1,                   data = sim_data)
fit_1 = lm(y ~ poly(x, degree = 1), data = sim_data)
fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
fit_9 = lm(y ~ poly(x, degree = 9), data = sim_data)
# Get predictions
predictions[sim, 1] = predict(fit_0, x)
predictions[sim, 2] = predict(fit_1, x)
predictions[sim, 3] = predict(fit_2, x)
predictions[sim, 4] = predict(fit_9, x)
}
predictions = (predictions)
colnames(predictions) = c("0", "1", "2", "9")
predictions = as.data.frame(predictions)
tall_predictions = tidyr::gather(predictions, factor_key = TRUE)
boxplot(value ~ key, data = tall_predictions, border = "darkgrey", xlab = "Polynomial Degree", ylab = "Predictions",
main = "Simulated Predictions for Polynomial Models")
grid()
stripchart(value ~ key, data = tall_predictions, add = TRUE, vertical = TRUE, method = "jitter", jitter = 0.15, pch = 1, col = c("dodgerblue", "firebrick", "springgreen", "darkorange"))
abline(h = f(x = 0.90), lwd = 2)
get_mse = function(truth, estimate) {
mean((estimate - truth) ^ 2)
}
get_bias = function(estimate, truth) {
mean(estimate) - truth
}
get_var = function(estimate) {
mean((estimate - mean(estimate)) ^ 2)
}
bias = apply(predictions, 2, get_bias, truth = f(x = 0.90))
variance = apply(predictions, 2, get_var)
mse = apply(predictions, 2, get_mse, truth = f(x = 0.90))
results = data.frame(
poly_degree = c(0, 1, 2, 9),
round(mse, 5),
round(bias ^ 2, 5),
round(variance, 5)
)
colnames(results) = c("Degree", "Mean Squared Error", "Bias Squared", "Variance")
rownames(results) = NULL
knitr::kable(results, booktabs = TRUE, escape = TRUE, align = "c")
all(diff(bias ^ 2) < 0)
all(diff(variance) > 0)
diff(mse) < 0
bias ^ 2 + variance == mse
all.equal(bias ^ 2 + variance, mse)
get_epe = function(realized, estimate) {
mean((realized - estimate) ^ 2)
}
y = rnorm(n = nrow(predictions), mean = f(x = 0.9), sd = 0.3)
epe = apply(predictions, 2, get_epe, realized = y)
epe
# hmmm, what's wrong here?
# the mean realative diff does go down with n
# is there really just that much compuational error?
sigma_hat = mean((y - f(x = 0.90)) ^ 2)
all.equal(epe, bias ^ 2 + variance + sigma_hat)
set.seed(1)
# Note this is intentionally incomplete
n_sims = 1000
X = runif(n = n_sims, min = 0, max = 1)
Y = rnorm(n = n_sims, mean = f(X), sd = 0.3)
f_hat_X = rep(0, length(X))
for (i in seq_along(X)) {
sim_data = get_sim_data(f)
fit_2 = lm(y ~ poly(x, degree = 2), data = sim_data)
f_hat_X[i] = predict(fit_2, newdata = data.frame(x = X[i]))
}
mean((Y - f_hat_X) ^ 2)
install.packages("FNN")
